#!/usr/bin/env python3
"""
Ù†Ø¸Ø§Ù… RAG Ù„Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ
Retrieval-Augmented Generation System for Palestinian Law
"""

import json
import pathlib
import re
from typing import List, Dict, Optional, Tuple
import numpy as np
from dataclasses import dataclass
import hashlib

try:
    from sentence_transformers import SentenceTransformer
    from sklearn.metrics.pairwise import cosine_similarity
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False
    print("âš ï¸  ØªØ«Ø¨ÙŠØª sentence-transformers Ù…Ø·Ù„ÙˆØ¨: pip install sentence-transformers")

@dataclass
class LawDocument:
    """ÙˆØ«ÙŠÙ‚Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©"""
    id: str
    title: str
    content: str
    source_url: str
    document_type: str  # 'law', 'decision', 'regulation'
    jurisdiction: str = "PS"
    issued_at: Optional[str] = None
    metadata: Dict = None

@dataclass
class SearchResult:
    """Ù†ØªÙŠØ¬Ø© Ø¨Ø­Ø«"""
    document: LawDocument
    similarity_score: float
    excerpt: str
    start_pos: int
    end_pos: int

class PalestinianLegalRAG:
    """Ù†Ø¸Ø§Ù… RAG Ù„Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ"""
    
    def __init__(self, corpus_path: str = "corpus.jsonl"):
        self.corpus_path = pathlib.Path(corpus_path)
        self.documents: List[LawDocument] = []
        self.embeddings = None
        self.model = None
        
        if RAG_AVAILABLE:
            self._load_model()
        
        self._load_corpus()
    
    def _load_model(self):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Embeddings Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©"""
        try:
            # Ù†Ù…ÙˆØ°Ø¬ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Embeddings Ø¨Ù†Ø¬Ø§Ø­")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
            self.model = None
    
    def _load_corpus(self):
        """ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ†"""
        if not self.corpus_path.exists():
            print(f"âŒ Ù…Ù„Ù Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {self.corpus_path}")
            return
        
        print(f"ğŸ“š ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ù…Ù† {self.corpus_path}")
        
        with open(self.corpus_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    data = json.loads(line.strip())
                    
                    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
                    content = self._clean_arabic_text(data.get('body', ''))
                    if len(content.strip()) < 100:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚ØµÙŠØ±Ø©
                        continue
                    
                    doc = LawDocument(
                        id=data.get('id', str(line_num)),
                        title=data.get('title', 'ÙˆØ«ÙŠÙ‚Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©'),
                        content=content,
                        source_url=data.get('source_url', ''),
                        document_type=data.get('type', 'law_or_regulation'),
                        jurisdiction=data.get('jurisdiction', 'PS'),
                        issued_at=data.get('issued_at'),
                        metadata={
                            'filename': data.get('filename'),
                            'text_file': data.get('text_file'),
                            'version': data.get('version', 1)
                        }
                    )
                    self.documents.append(doc)
                    
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø³Ø·Ø± {line_num}: {e}")
                    continue
        
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(self.documents)} ÙˆØ«ÙŠÙ‚Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©")
    
    def _clean_arabic_text(self, text: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ"""
        if not text:
            return ""
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø­Ø±Ù Ø§Ù„Ø®Ø§ØµØ© ÙˆØ§Ù„Ø±Ù…ÙˆØ²
        text = re.sub(r'[^\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFF\s\w\d\.\,\;\:\!\?\(\)\[\]\{\}]', ' ', text)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
        text = re.sub(r'\s+', ' ', text)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ÙØ§Ø±ØºØ© Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
        text = re.sub(r'\n\s*\n', '\n', text)
        
        return text.strip()
    
    def _create_chunks(self, text: str, chunk_size: int = 512, overlap: int = 50) -> List[str]:
        """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚"""
        if len(text) <= chunk_size:
            return [text]
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ù‚Ø·Ø¹ Ø¹Ù†Ø¯ Ù†Ù‡Ø§ÙŠØ© Ø¬Ù…Ù„Ø©
            if end < len(text):
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù‚Ø±Ø¨ Ù†Ù‚Ø·Ø© Ø£Ùˆ Ø¹Ù„Ø§Ù…Ø© Ø§Ø³ØªÙÙ‡Ø§Ù…
                for punct in ['.', 'ØŸ', '!', '\n']:
                    last_punct = text.rfind(punct, start, end)
                    if last_punct > start + chunk_size // 2:
                        end = last_punct + 1
                        break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - overlap
            if start >= len(text):
                break
        
        return chunks
    
    def _extract_excerpt(self, text: str, query: str, max_length: int = 200) -> Tuple[str, int, int]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù‚ØªØ·Ù Ø°Ùˆ ØµÙ„Ø© Ø¨Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…"""
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ÙÙŠ Ø§Ù„Ù†Øµ
        query_words = query.split()
        
        best_pos = 0
        best_score = 0
        
        for i in range(0, len(text) - max_length, 50):
            excerpt = text[i:i + max_length]
            score = sum(1 for word in query_words if word in excerpt)
            
            if score > best_score:
                best_score = score
                best_pos = i
        
        start_pos = best_pos
        end_pos = min(best_pos + max_length, len(text))
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ù‚Ø·Ø¹ Ø¹Ù†Ø¯ Ù†Ù‡Ø§ÙŠØ© Ø¬Ù…Ù„Ø©
        for punct in ['.', 'ØŸ', '!', '\n']:
            last_punct = text.rfind(punct, start_pos, end_pos)
            if last_punct > start_pos + max_length // 2:
                end_pos = last_punct + 1
                break
        
        return text[start_pos:end_pos].strip(), start_pos, end_pos
    
    def build_embeddings(self):
        """Ø¨Ù†Ø§Ø¡ Embeddings Ù„Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©"""
        if not self.model or not self.documents:
            print("âŒ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£Ùˆ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©")
            return
        
        print("ğŸ”§ Ø¨Ù†Ø§Ø¡ Embeddings Ù„Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©...")
        
        all_chunks = []
        chunk_to_doc = []
        
        for doc in self.documents:
            chunks = self._create_chunks(doc.content)
            all_chunks.extend(chunks)
            chunk_to_doc.extend([doc] * len(chunks))
        
        # Ø¥Ù†Ø´Ø§Ø¡ Embeddings
        embeddings = self.model.encode(all_chunks, show_progress_bar=True)
        
        self.embeddings = embeddings
        self.chunk_to_doc = chunk_to_doc
        self.all_chunks = all_chunks
        
        print(f"âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Embeddings Ù„Ù€ {len(all_chunks)} Ø¬Ø²Ø¡")
    
    def search(self, query: str, top_k: int = 5) -> List[SearchResult]:
        """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù‚ÙˆØ§Ù†ÙŠÙ†"""
        if not self.embeddings is not None:
            print("âŒ ÙŠØ¬Ø¨ Ø¨Ù†Ø§Ø¡ Embeddings Ø£ÙˆÙ„Ø§Ù‹")
            return []
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        clean_query = self._clean_arabic_text(query)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Embedding Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        query_embedding = self.model.encode([clean_query])
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        similarities = cosine_similarity(query_embedding, self.embeddings)[0]
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = []
        seen_docs = set()
        
        for idx in top_indices:
            doc = self.chunk_to_doc[idx]
            chunk = self.all_chunks[idx]
            score = similarities[idx]
            
            # ØªØ¬Ù†Ø¨ ØªÙƒØ±Ø§Ø± Ù†ÙØ³ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©
            if doc.id in seen_docs:
                continue
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù‚ØªØ·Ù Ø°Ùˆ ØµÙ„Ø©
            excerpt, start_pos, end_pos = self._extract_excerpt(chunk, clean_query)
            
            result = SearchResult(
                document=doc,
                similarity_score=float(score),
                excerpt=excerpt,
                start_pos=start_pos,
                end_pos=end_pos
            )
            
            results.append(result)
            seen_docs.add(doc.id)
            
            if len(results) >= top_k:
                break
        
        return results
    
    def get_legal_context(self, query: str, max_results: int = 3) -> Dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…"""
        results = self.search(query, top_k=max_results)
        
        context = {
            'relevant_laws': [],
            'court_decisions': [],
            'legal_principles': [],
            'total_documents': len(self.documents)
        }
        
        for result in results:
            doc_info = {
                'title': result.document.title,
                'excerpt': result.excerpt,
                'source_url': result.document.source_url,
                'similarity_score': result.similarity_score,
                'document_type': result.document.document_type
            }
            
            if result.document.document_type in ['law_or_regulation', 'law']:
                context['relevant_laws'].append(doc_info)
            elif result.document.document_type in ['web_page', 'decision']:
                context['court_decisions'].append(doc_info)
            else:
                context['legal_principles'].append(doc_info)
        
        return context
    
    def add_document(self, title: str, content: str, source_url: str, doc_type: str = "law"):
        """Ø¥Ø¶Ø§ÙØ© ÙˆØ«ÙŠÙ‚Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©"""
        doc_id = hashlib.sha1(f"{title}{source_url}".encode()).hexdigest()
        
        doc = LawDocument(
            id=doc_id,
            title=title,
            content=self._clean_arabic_text(content),
            source_url=source_url,
            document_type=doc_type
        )
        
        self.documents.append(doc)
        print(f"âœ… ØªÙ… Ø¥Ø¶Ø§ÙØ© ÙˆØ«ÙŠÙ‚Ø© Ø¬Ø¯ÙŠØ¯Ø©: {title}")
    
    def save_corpus(self, output_path: str = None):
        """Ø­ÙØ¸ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ù…Ø­Ø¯Ø«Ø©"""
        if not output_path:
            output_path = self.corpus_path
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for doc in self.documents:
                data = {
                    'id': doc.id,
                    'title': doc.title,
                    'body': doc.content,
                    'source_url': doc.source_url,
                    'type': doc.document_type,
                    'jurisdiction': doc.jurisdiction,
                    'issued_at': doc.issued_at,
                    'metadata': doc.metadata or {}
                }
                f.write(json.dumps(data, ensure_ascii=False) + '\n')
        
        print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© ÙÙŠ {output_path}")


def main():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù…"""
    print("ğŸ›ï¸  Ù†Ø¸Ø§Ù… RAG Ù„Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„ÙÙ„Ø³Ø·ÙŠÙ†ÙŠ")
    print("=" * 50)
    
    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
    rag = PalestinianLegalRAG("out/corpus.jsonl")
    
    if not RAG_AVAILABLE:
        print("âŒ ØªØ«Ø¨ÙŠØª sentence-transformers Ù…Ø·Ù„ÙˆØ¨ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
        return
    
    # Ø¨Ù†Ø§Ø¡ Embeddings
    rag.build_embeddings()
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¨Ø­Ø«
    test_queries = [
        "Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…Ù„ÙƒÙŠØ© Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠØ©",
        "Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ø¹Ù…Ø§Ù„",
        "Ø§Ù„Ø·Ù„Ø§Ù‚ ÙˆØ§Ù„Ø®Ù„Ø¹",
        "Ø§Ù„Ø¬Ø±Ø§Ø¦Ù… Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯ÙŠØ©"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†: {query}")
        results = rag.search(query, top_k=3)
        
        for i, result in enumerate(results, 1):
            print(f"  {i}. {result.document.title}")
            print(f"     Ø§Ù„ØªØ´Ø§Ø¨Ù‡: {result.similarity_score:.3f}")
            print(f"     Ø§Ù„Ù…Ù‚ØªØ·Ù: {result.excerpt[:100]}...")
            print()


if __name__ == "__main__":
    main()
